{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae6e052-79ad-4753-8cd8-e4a088a2c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6\"\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)\n",
    "import optuna\n",
    "import math\n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "from autogluon.common.savers import save_pd\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec9e409-a779-4f94-944a-8bcb0eeaac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_d = TabularDataset('C:/Users/16010/Desktop/Deep learning from Scratch/kaggle-S4E7/data/train.csv')\n",
    "te_d = TabularDataset('C:/Users/16010/Desktop/Deep learning from Scratch/kaggle-S4E7/data/test.csv')\n",
    "tr_d = tr_d.drop(\"id\",axis=1)\n",
    "uid = te_d['id']\n",
    "te_d = te_d.drop(\"id\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ac6f60-0a97-4eb3-9186-666702741b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>65101.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>58911.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>288</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>38043.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>31951.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  Driving_License  Region_Code  Previously_Insured Vehicle_Age  \\\n",
       "0    Male   21                1         35.0                   0    1-2 Year   \n",
       "1    Male   43                1         28.0                   0   > 2 Years   \n",
       "2  Female   25                1         14.0                   1    < 1 Year   \n",
       "3  Female   35                1          1.0                   0    1-2 Year   \n",
       "4  Female   36                1         15.0                   1    1-2 Year   \n",
       "\n",
       "  Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Vintage  Response  \n",
       "0            Yes         65101.0                 124.0      187         0  \n",
       "1            Yes         58911.0                  26.0      288         1  \n",
       "2             No         38043.0                 152.0      254         0  \n",
       "3            Yes          2630.0                 156.0       76         0  \n",
       "4             No         31951.0                 152.0      294         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15a02eb-bbb0-4e51-98a0-50836d9d802f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>37483.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2630.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Female</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>24502.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>34115.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Age  Driving_License  Region_Code  Previously_Insured Vehicle_Age  \\\n",
       "0  Female   20                1         47.0                   0    < 1 Year   \n",
       "1    Male   47                1         28.0                   0    1-2 Year   \n",
       "2    Male   47                1         43.0                   0    1-2 Year   \n",
       "3  Female   22                1         47.0                   1    < 1 Year   \n",
       "4    Male   51                1         19.0                   0    1-2 Year   \n",
       "\n",
       "  Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Vintage  \n",
       "0             No          2630.0                 160.0      228  \n",
       "1            Yes         37483.0                 124.0      123  \n",
       "2            Yes          2630.0                  26.0      271  \n",
       "3             No         24502.0                 152.0      115  \n",
       "4             No         34115.0                 124.0      148  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa03321-d300-4e39-b8bb-eba666f7de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \n",
    "    \"XGB\": {\n",
    "        \n",
    "        \"colsample_bytree\": 0.516652313273348,\n",
    "        \"enable_categorical\": True,\n",
    "        \"learning_rate\": 0.015,\n",
    "        \"max_depth\": 9,\n",
    "        \"min_child_weight\": 0.8567068904025429,\n",
    "        \"max_bin\": 262143,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"tree_method\": \"gpu_hist\",  # 使用GPU\n",
    "        \"predictor\": \"gpu_predictor\" \n",
    "          },\n",
    "    \n",
    "    'GBM': {\n",
    "          \n",
    "        'n_estimators': 1190,\n",
    "        'learning_rate': 0.22952000374471332,\n",
    "        'max_depth': 13, \n",
    "        'reg_alpha': 8.200152384535924,\n",
    "        'reg_lambda': 4.285393733702208, \n",
    "        'num_leaves': 100, \n",
    "        'subsample': 0.6497981764924947, \n",
    "        'colsample_bytree': 0.37368304607248115,\n",
    "        'device': 'gpu'  \n",
    "           } \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f4a16ee-cda5-4a3f-a830-3ffd56a1b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autogluon.core.dataset.TabularDataset'>\n",
      "RangeIndex: 11504798 entries, 0 to 11504797\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   Gender                object \n",
      " 1   Age                   int64  \n",
      " 2   Driving_License       int64  \n",
      " 3   Region_Code           float64\n",
      " 4   Previously_Insured    int64  \n",
      " 5   Vehicle_Age           object \n",
      " 6   Vehicle_Damage        object \n",
      " 7   Annual_Premium        float64\n",
      " 8   Policy_Sales_Channel  float64\n",
      " 9   Vintage               int64  \n",
      " 10  Response              int64  \n",
      "dtypes: float64(3), int64(5), object(3)\n",
      "memory usage: 965.5+ MB\n",
      "None\n",
      "                Age  Driving_License   Region_Code  Previously_Insured  \\\n",
      "count  1.150480e+07     1.150480e+07  1.150480e+07        1.150480e+07   \n",
      "mean   3.838356e+01     9.980220e-01  2.641869e+01        4.629966e-01   \n",
      "std    1.499346e+01     4.443120e-02  1.299159e+01        4.986289e-01   \n",
      "min    2.000000e+01     0.000000e+00  0.000000e+00        0.000000e+00   \n",
      "25%    2.400000e+01     1.000000e+00  1.500000e+01        0.000000e+00   \n",
      "50%    3.600000e+01     1.000000e+00  2.800000e+01        0.000000e+00   \n",
      "75%    4.900000e+01     1.000000e+00  3.500000e+01        1.000000e+00   \n",
      "max    8.500000e+01     1.000000e+00  5.200000e+01        1.000000e+00   \n",
      "\n",
      "       Annual_Premium  Policy_Sales_Channel       Vintage      Response  \n",
      "count    1.150480e+07          1.150480e+07  1.150480e+07  1.150480e+07  \n",
      "mean     3.046137e+04          1.124254e+02  1.638977e+02  1.229973e-01  \n",
      "std      1.645475e+04          5.403571e+01  7.997953e+01  3.284341e-01  \n",
      "min      2.630000e+03          1.000000e+00  1.000000e+01  0.000000e+00  \n",
      "25%      2.527700e+04          2.900000e+01  9.900000e+01  0.000000e+00  \n",
      "50%      3.182400e+04          1.510000e+02  1.660000e+02  0.000000e+00  \n",
      "75%      3.945100e+04          1.520000e+02  2.320000e+02  0.000000e+00  \n",
      "max      5.401650e+05          1.630000e+02  2.990000e+02  1.000000e+00  \n"
     ]
    }
   ],
   "source": [
    "print(tr_d.info())\n",
    "print(tr_d.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb877804-2f3f-4078-8e40-f749df2969a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240705_004629\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.1\n",
      "Python Version:     3.10.14\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          20\n",
      "Memory Avail:       14.94 GB / 31.84 GB (46.9%)\n",
      "Disk Space Avail:   81.74 GB / 729.84 GB (11.2%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 5400s of the 21600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2024-07-04 20:46:32,346\tINFO worker.py:1743 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"AutogluonModels\\ag-20240705_004629\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Beginning AutoGluon training ... Time limit = 5395s\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m AutoGluon will save models to \"AutogluonModels\\ag-20240705_004629\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Train Data Rows:    10226487\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Train Data Columns: 10\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Label Column:       Response\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tAvailable Memory:                    14230.03 MB\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tTrain Data (Original)  Memory Usage: 2364.66 MB (16.6% of available memory)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tWarning: Data size prior to feature transformation consumes 16.6% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t('float', [])  : 3 | ['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t('int', [])    : 4 | ['Age', 'Driving_License', 'Previously_Insured', 'Vintage']\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t('object', []) : 3 | ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t('category', [])  : 1 | ['Vehicle_Age']\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t('float', [])     : 3 | ['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t('int', [])       : 2 | ['Age', 'Vintage']\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\t('int', ['bool']) : 4 | ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Damage']\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t13.4s = Fit runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t10 features in original data used to generate 10 features in processed data.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tTrain Data (Processed) Memory Usage: 438.87 MB (3.0% of available memory)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Data preprocessing and feature engineering runtime = 14.47s ...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t'XGB': {'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.015, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'max_bin': 262143, 'n_estimators': 10000, 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'},\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t'GBM': {'n_estimators': 1190, 'learning_rate': 0.22952000374471332, 'max_depth': 13, 'reg_alpha': 8.200152384535924, 'reg_lambda': 4.285393733702208, 'num_leaves': 100, 'subsample': 0.6497981764924947, 'colsample_bytree': 0.37368304607248115, 'device': 'gpu'},\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Included models: ['GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting 2 L1 models ...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 3586.06s of the 5380.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 28.76% memory usage per fold, 57.52%/80.00% total).\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=10, gpus=0, memory=28.76%)\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=2328)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=2328)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=2328)\u001b[0m 1 warning generated.\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48956)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=48956)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=48956)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=48956)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=48956)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=48956)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=48956)\u001b[0m 1 warning generated.\u001b[32m [repeated 55x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=2328)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=53708)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=53708)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.right_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 856 .\n",
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=27564)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=53708)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=43300)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=43300)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=6152)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.251056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=43300)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=49780)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.right_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 856 .\n",
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=29076)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.250877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t0.8813\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t1021.0s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t83.33s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 2551.34s of the 4345.72s of remaining time.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 20.805 GB out of 12.067 GB available memory (172.416%)... (100.000% of avail memory is the max safe size)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.77 to avoid the error)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tNot enough memory to train XGBoost_BAG_L1... Skipping this model.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 4343.56s of remaining time.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t0.8813\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t0.4s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t1.58s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Included models: ['GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting 2 L2 models ...\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 4341.25s of the 4341.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 33.67% memory usage per fold, 67.34%/80.00% total).\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=10, gpus=0, memory=33.67%)\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=50180)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=50180)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=50180)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=50180)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=50180)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=56996)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=56996)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=56996)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=56996)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=56996)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=15732)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15732)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15732)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=15732)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=15732)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=15732)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=15732)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=43692)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "\u001b[36m(_ray_fit pid=43692)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "\u001b[36m(_ray_fit pid=43692)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=43692)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=43692)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=55644)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=55644)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=55644)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=55644)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=55644)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=56516)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=56516)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=56516)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[33m(raylet)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(_ray_fit pid=56516)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=56516)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t0.8812\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t99.95s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t4.41s\t = Validation runtime\n",
      "\u001b[36m(_ray_fit pid=16660)\u001b[0m [LightGBM] [Fatal] Check failed: (best_split_info.left_count) > (0) at D:\\a\\1\\s\\lightgbm-python\\src\\treelearner\\serial_tree_learner.cpp, line 846 .\n",
      "\u001b[36m(_ray_fit pid=16660)\u001b[0m \n",
      "\u001b[36m(_ray_fit pid=16660)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=16660)\u001b[0m C:\\Users\\16010\\.conda\\envs\\3.10forkaggleS4E7\\lib\\site-packages\\lightgbm\\engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=16660)\u001b[0m   _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 4233.79s of the 4233.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tWarning: Not enough memory to safely train model. Estimated to require 22.763 GB out of 11.956 GB available memory (190.386%)... (100.000% of avail memory is the max safe size)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.95 to avoid the error)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tNot enough memory to train XGBoost_BAG_L2... Skipping this model.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 434.12s of the 4229.62s of remaining time.\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 0.5, 'LightGBM_BAG_L2': 0.5}\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t0.8813\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t6.43s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m \t1.59s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m AutoGluon training complete, total runtime = 1174.14s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 14535.4 rows/s (1278311 batch size)\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240705_004629\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=41496)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      LightGBM_BAG_L2       0.882022   0.881242     roc_auc       61.761691      87.746082  1120.949933                 3.422865                4.411098          99.952925            2       True          3\n",
      "1  WeightedEnsemble_L3       0.882000   0.881289     roc_auc       61.779692      89.335733  1127.377817                 0.018001                1.589652           6.427884            3       True          4\n",
      "2      LightGBM_BAG_L1       0.881937   0.881260     roc_auc       58.338825      83.334984  1020.997008                58.338825               83.334984        1020.997008            1       True          1\n",
      "3  WeightedEnsemble_L2       0.881937   0.881260     roc_auc       58.363378      84.918725  1021.395390                 0.024552                1.583742           0.398382            2       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1249s\t = DyStack   runtime |\t20351s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 20351s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240705_004629\"\n",
      "Train Data Rows:    11504798\n",
      "Train Data Columns: 10\n",
      "Label Column:       Response\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16146.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 2660.24 MB (16.5% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 16.5% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 3 | ['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']\n",
      "\t\t('int', [])    : 4 | ['Age', 'Driving_License', 'Previously_Insured', 'Vintage']\n",
      "\t\t('object', []) : 3 | ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 1 | ['Vehicle_Age']\n",
      "\t\t('float', [])     : 3 | ['Region_Code', 'Annual_Premium', 'Policy_Sales_Channel']\n",
      "\t\t('int', [])       : 2 | ['Age', 'Vintage']\n",
      "\t\t('int', ['bool']) : 4 | ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Damage']\n",
      "\t14.9s = Fit runtime\n",
      "\t10 features in original data used to generate 10 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 493.73 MB (3.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 15.9s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'XGB': {'colsample_bytree': 0.516652313273348, 'enable_categorical': True, 'learning_rate': 0.015, 'max_depth': 9, 'min_child_weight': 0.8567068904025429, 'max_bin': 262143, 'n_estimators': 10000, 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor'},\n",
      "\t'GBM': {'n_estimators': 1190, 'learning_rate': 0.22952000374471332, 'max_depth': 13, 'reg_alpha': 8.200152384535924, 'reg_lambda': 4.285393733702208, 'num_leaves': 100, 'subsample': 0.6497981764924947, 'colsample_bytree': 0.37368304607248115, 'device': 'gpu'},\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Included models: ['GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 2 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 13553.12s of the 20334.77s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 23.71% memory usage per fold, 47.43%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=10, gpus=0, memory=23.71%)\n",
      "\t0.8813\t = Validation score   (roc_auc)\n",
      "\t1079.96s\t = Training   runtime\n",
      "\t105.18s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 12455.72s of the 19237.36s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 20.518 GB out of 14.201 GB available memory (144.484%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.49 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train XGBoost_BAG_L1... Skipping this model.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1355.31s of the 19234.93s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.8813\t = Validation score   (roc_auc)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t1.79s\t = Validation runtime\n",
      "Included models: ['GBM', 'XGB'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Fitting 2 L2 models ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 19232.31s of the 19232.13s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.05% memory usage per fold, 54.10%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=10, gpus=0, memory=27.05%)\n",
      "\t0.8814\t = Validation score   (roc_auc)\n",
      "\t102.94s\t = Training   runtime\n",
      "\t4.61s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 19121.21s of the 19121.02s of remaining time.\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 22.520 GB out of 13.584 GB available memory (165.787%)... (100.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.71 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train XGBoost_BAG_L2... Skipping this model.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 1923.23s of the 19116.6s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.667, 'LightGBM_BAG_L1': 0.333}\n",
      "\t0.8814\t = Validation score   (roc_auc)\n",
      "\t6.4s\t = Training   runtime\n",
      "\t1.77s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1243.0s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 13072.1 rows/s (1438100 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240705_004629\")\n"
     ]
    }
   ],
   "source": [
    "included_models = [\"GBM\", \"XGB\",]\n",
    "\n",
    "PREDICTOR_AUTO = TabularPredictor(label = 'Response'\n",
    "                                  ,eval_metric = 'roc_auc',).fit(\n",
    "        \n",
    "        train_data = tr_d,\n",
    "        \n",
    "        hyperparameters = params,\n",
    "        \n",
    "        time_limit = 3600*6,  \n",
    "        \n",
    "        presets='best_quality',\n",
    "    \n",
    "        included_model_types = included_models,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b608b9fa-77e7-4235-b2bb-3492c7e220ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>0.881399</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>111.559919</td>\n",
       "      <td>1189.294574</td>\n",
       "      <td>1.768074</td>\n",
       "      <td>6.400234</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM_BAG_L2</td>\n",
       "      <td>0.881374</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>109.791845</td>\n",
       "      <td>1182.894341</td>\n",
       "      <td>4.607082</td>\n",
       "      <td>102.935292</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.881345</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>105.184763</td>\n",
       "      <td>1079.959049</td>\n",
       "      <td>105.184763</td>\n",
       "      <td>1079.959049</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.881345</td>\n",
       "      <td>roc_auc</td>\n",
       "      <td>106.979437</td>\n",
       "      <td>1080.385511</td>\n",
       "      <td>1.794674</td>\n",
       "      <td>0.426462</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_val eval_metric  pred_time_val     fit_time  \\\n",
       "0  WeightedEnsemble_L3   0.881399     roc_auc     111.559919  1189.294574   \n",
       "1      LightGBM_BAG_L2   0.881374     roc_auc     109.791845  1182.894341   \n",
       "2      LightGBM_BAG_L1   0.881345     roc_auc     105.184763  1079.959049   \n",
       "3  WeightedEnsemble_L2   0.881345     roc_auc     106.979437  1080.385511   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                1.768074           6.400234            3       True   \n",
       "1                4.607082         102.935292            2       True   \n",
       "2              105.184763        1079.959049            1       True   \n",
       "3                1.794674           0.426462            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          4  \n",
       "1          3  \n",
       "2          1  \n",
       "3          2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTOR_AUTO.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daba7f3e-56fa-4eae-bd01-f992e19256ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM version: 4.3.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lightgbm.basic' has no attribute '_is_lib_gpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM version:\u001b[39m\u001b[38;5;124m'\u001b[39m, lgb\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLightGBM built with GPU support:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_lib_gpu\u001b[49m())\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lightgbm.basic' has no attribute '_is_lib_gpu'"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "print('LightGBM version:', lgb.__version__)\n",
    "print('LightGBM built with GPU support:', lgb.basic._is_lib_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06684ec5-9af5-43f9-9955-37e868758046",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = PREDICTOR_AUTO.predict(te_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c65c572-ea4b-4c10-bde2-1603123efd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': uid,  # 确保您的测试数据中包含'id'列\n",
    "    'Response': predictions\n",
    "})\n",
    "submission.to_csv('C:/Users/16010/Desktop/Deep learning from Scratch/kaggle-S4E7/submission/submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b032905-cd11-498c-8cef-6bebd3c34784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
